{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Structured Streaming: convirtiendo consultas batch en streaming"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n<p><b>PARA SABER M\u00c1S</b>: Notebook interesante sobre Structured Streaming hecho por Databricks \n    <a target = \"_blank\" href=\"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html\">aqu\u00ed</a>\n</p>\n</div>\n"}, {"cell_type": "markdown", "metadata": {}, "source": "## Descripci\u00f3n de las variables"}, {"cell_type": "markdown", "metadata": {}, "source": "El dataset est\u00e1 compuesto por las siguientes variables:\n\n1. **Year** 2008\n2. **Month** 1\n3. **DayofMonth** 1-31\n4. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n5. **DepTime** hora real de salida (local, hhmm)\n6. **CRSDepTime** hora prevista de salida (local, hhmm)\n7. **ArrTime** hora real de llegada (local, hhmm)\n8. **CRSArrTime** hora prevista de llegada (local, hhmm)\n9. **UniqueCarrier** c\u00f3digo del aparato\n10. **FlightNum** n\u00famero de vuelo\n11. **TailNum** identificador de cola: aircraft registration, unique aircraft identifier\n12. **ActualElapsedTime** tiempo real invertido en el vuelo\n13. **CRSElapsedTime** en minutos\n14. **AirTime** en minutos\n15. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterriz\u00f3 menos de 15 minutos m\u00e1s tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n16. **DepDelay** retraso a la salida, en minutos\n17. **Origin** c\u00f3digo IATA del aeropuerto de origen\n18. **Dest** c\u00f3digo IATA del aeropuerto de destino\n19. **Distance** en millas\n20. **TaxiIn** taxi in time, in minutes\n21. **TaxiOut** taxi out time in minutes\n22. **Cancelled** *si el vuelo fue cancelado (1 = s\u00ed, 0 = no)\n23. **CancellationCode** raz\u00f3n de cancelaci\u00f3n (A = aparato, B = tiempo atmosf\u00e9rico, C = NAS, D = seguridad)\n24. **Diverted** *si el vuelo ha sido desviado (1 = s\u00ed, 0 = no)\n25. **CarrierDelay** en minutos: El retraso del transportista est\u00e1 bajo el control del transportista a\u00e9reo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, da\u00f1o de la aeronave, espera de la llegada de los pasajeros o la tripulaci\u00f3n de conexi\u00f3n, equipaje, impacto de un p\u00e1jaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulaci\u00f3n (descanso del piloto o acompa\u00f1ante) , da\u00f1os por mercanc\u00edas peligrosas, inspecci\u00f3n de ingenier\u00eda, abastecimiento de combustible, pasajeros discapacitados, tripulaci\u00f3n retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegaci\u00f3n de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no v\u00e1lido, retrasos de peso y equilibrio.\n26. **WeatherDelay** en minutos: causado por condiciones atmosf\u00e9ricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n27. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorol\u00f3gicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tr\u00e1fico a\u00e9reo, problemas con los controladores a\u00e9reos, etc.\n28. **SecurityDelay** en minutos: causado por la evacuaci\u00f3n de una terminal, re-embarque de un avi\u00f3n debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n29. **LateAircraftDelay** en minutos: debido al propio retraso del avi\u00f3n al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora m\u00e1s tard\u00eda de la que estaba prevista."}, {"cell_type": "markdown", "metadata": {}, "source": "#### Descargamos una versi\u00f3n del dataset reducido dividida en 10 ficheros peque\u00f1os y los subimos a la carpeta /tmp/flightsFolder de HDFS"}, {"cell_type": "markdown", "metadata": {}, "source": "**Vamos a simular** procesamiento en Streaming leyendo cada vez un fichero de esa carpeta, de los 10 existentes, como si hubiese un proceso externo que los va creando en esa carpeta (aunque ya existan todos desde el principio; por eso los vamos procesando uno a uno). Pod\u00e9is probar a no subir los 10 sino subir solo 3 o 4, y despu\u00e9s de un rato, en cualquier momento subir m\u00e1s ficheros."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!wget https://github.com/olbapjose/xapi-clojure/raw/master/flightsFolder.zip\n!unzip flightsFolder.zip\n!hdfs dfs -copyFromLocal flightsFolder /tmp"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!hdfs dfs -ls /tmp/flightsFolder"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Descargamos tambi\u00e9n una versi\u00f3n completa de ese dataset, que todav\u00eda no incluye la columna ArrDelayCat que le vamos a a\u00f1adir a continuaci\u00f3n. Los 10 ficheros anteriores s\u00ed tienen ya esa columna y siguen el mismo esquema que este fichero. El objetivo de descargar este fichero es para aprovechar su esquema y no tener que construir nosotros a mano un esquema"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "!wget https://raw.githubusercontent.com/olbapjose/xapi-clojure/master/flights_jan08.csv\n!hdfs dfs -copyFromLocal flights_jan08.csv /tmp"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "import pyspark.sql.functions as F\nfrom pyspark.sql.types import IntegerType\n\n# Leemos los datos, quitamos filas con NA y convertimos a num\u00e9rico\nflightsDF = spark.read\\\n                 .option(\"header\", \"true\")\\\n                 .option(\"inferSchema\", \"true\")\\\n                 .csv(\"flights_jan08.csv\")\n\ncleanFlightsDF = flightsDF.where(\"ArrDelay != 'NA' and DepDelay != 'NA'\")\\\n                          .withColumn(\"ArrDelay\", F.col(\"ArrDelay\").cast(IntegerType()))\\\n                          .withColumn(\"DepDelay\", F.col(\"DepDelay\").cast(IntegerType()))\\\n                          .withColumn(\"ArrDelayCat\", F.when(F.col(\"ArrDelay\") < 15, \"None\")\\\n                                                      .when((F.col(\"ArrDelay\") >= 15) & (F.col(\"ArrDelay\") < 60), \"Slight\")\\\n                                                      .otherwise(\"Huge\"))\\\n                          .cache() # we will be working with it from now on!"}, {"cell_type": "markdown", "metadata": {}, "source": "### Agregaciones en streaming"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<p><b>PREGUNTA</b>: \u00bfCu\u00e1l es el retraso medio por cada destino para vuelos que salen de SFO?\n    Convierte esta consulta en streaming</p>\n</div>"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "flightsSchema = cleanFlightsDF.schema # in Structured Streaming the schema is mandatory\n\nstreamingFlights = spark.readStream.schema(flightsSchema)\\\n                        .option(\"maxFilesPerTrigger\", 1)\\\n                        .csv(\"/tmp/flightsFolder\") # leer un solo fichero nuevo en cada operaci\u00f3n de lectura de esta carpeta\n\n# Operaci\u00f3n de agregaci\u00f3n, igual que con un DataFrame convencional\n# COMPLETA LA CONSULTA: de los vuelos que salen de SFO, calcular el retraso medio para cada destino\nlargestAverageSFOstreamingDF = streamingFlights.<COMPLETAR>\n\n# Ahora escribimos continuamente el resultado. Solo para test, escribimos en memoria\ncountQuery = largestAverageSFOstreamingDF\\\n                .writeStream.queryName(\"meanArrDelaySFO\")\\\n                .format(\"memory\")\\\n                .outputMode(\"complete\")\\\n                .start() # esto lanza el stream\n\n# Puesto que el driver es este notebook de Jupyter y no lo pensamos cerrar,\n# hasta que hayamos visualizado correctamente todas las salidas, entonces podemos omitir la l\u00ednea siguiente\n\n#countQuery.awaitTermination() # obligatorio en aplicaciones en producci\u00f3n para evitar que finalice el Driver\n\nimport time \n\nresultDF = spark.sql(\"select * from meanArrDelaySFO\")\n\nresultDF.show()\n\ntime.sleep(15)\n\nresultDF.show()\n\ntime.sleep(10)\n\nresultDF.show()\n\ntime.sleep(5)\n\nresultDF.show()\n\ntime.sleep(5)\n\nresultDF.show()\n\ntime.sleep(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Escribir en memoria implica que se crea una tabla autom\u00e1ticamente en memoria con el nombre de la consulta"}, {"cell_type": "markdown", "metadata": {}, "source": "En la celda anterior, lo que hemos hecho es que vamos consultando peri\u00f3dicamente el contenido de esa tabla y vemos que cada vez es distinto (va cambiando a lo largo del tiempo a medida que Spark va procesando nuevos ficheros y actualizando el resultado en la tabla)"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}