{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones avanzadas con DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descripción de las variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset, obtenido de <a target = \"_blank\" href=\"https://www.transtats.bts.gov/Fields.asp?Table_ID=236\">este link</a> está compuesto por las siguientes variables referidas siempre al año 2018:\n",
    "\n",
    "1. **Month** 1-4\n",
    "2. **DayofMonth** 1-31\n",
    "3. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n",
    "4. **FlightDate** fecha del vuelo\n",
    "5. **Origin** código IATA del aeropuerto de origen\n",
    "6. **OriginCity** ciudad donde está el aeropuerto de origen\n",
    "7. **Dest** código IATA del aeropuerto de destino\n",
    "8. **DestCity** ciudad donde está el aeropuerto de destino  \n",
    "9. **DepTime** hora real de salida (local, hhmm)\n",
    "10. **DepDelay** retraso a la salida, en minutos\n",
    "11. **ArrTime** hora real de llegada (local, hhmm)\n",
    "12. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterrizó menos de 15 minutos más tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n",
    "13. **Cancelled** si el vuelo fue cancelado (1 = sí, 0 = no)\n",
    "14. **CancellationCode** razón de cancelación (A = aparato, B = tiempo atmosférico, C = NAS, D = seguridad)\n",
    "15. **Diverted** si el vuelo ha sido desviado (1 = sí, 0 = no)\n",
    "16. **ActualElapsedTime** tiempo real invertido en el vuelo\n",
    "17. **AirTime** en minutos\n",
    "18. **Distance** en millas\n",
    "19. **CarrierDelay** en minutos: El retraso del transportista está bajo el control del transportista aéreo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, daño de la aeronave, espera de la llegada de los pasajeros o la tripulación de conexión, equipaje, impacto de un pájaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulación (descanso del piloto o acompañante) , daños por mercancías peligrosas, inspección de ingeniería, abastecimiento de combustible, pasajeros discapacitados, tripulación retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegación de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no válido, retrasos de peso y equilibrio.\n",
    "20. **WeatherDelay** en minutos: causado por condiciones atmosféricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n",
    "21. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorológicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tráfico aéreo, problemas con los controladores aéreos, etc.\n",
    "22. **SecurityDelay** en minutos: causado por la evacuación de una terminal, re-embarque de un avión debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n",
    "23. **LateAircraftDelay** en minutos: debido al propio retraso del avión al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora más tardía de la que estaba prevista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"OPAvanzadas\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Leemos los datos y quitamos filas con NA y convertimos a numéricas las columnas inferidas incorrectamente\n",
    "flightsDF = spark.read\\\n",
    "                 .option(\"header\", \"true\")\\\n",
    "                 .option(\"inferSchema\", \"true\")\\\n",
    "                 .csv(\"flights-jan-apr-2018.csv\")\n",
    "\n",
    "# Convertimos a enteros y re-categorizamos ArrDelay en una nueva columna ArrDelayCat\n",
    "# None (< 15 min), Slight(entre 15 y 60 min), Huge (> 60 min)\n",
    "\n",
    "cleanFlightsDF = flightsDF.withColumn(\"ArrDelayCat\", F.when(F.col(\"ArrDelay\") < 15, \"None\")\\\n",
    "                                                      .when((F.col(\"ArrDelay\") >= 15) & (F.col(\"ArrDelay\") < 60), \"Slight\")\\\n",
    "                                                      .otherwise(\"Huge\"))\\\n",
    "                           .cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hagamos algunas preguntas a los datos para obtener conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imaginemos que somos los dueños de una web de viajes que rastrea internet en busca de vuelos en agencias y otras páginas, los compara y recomienda el más adecuado para el aeropuerto. Junto con esta recomendación, querríamos dar también información sobre vuelos fiables y no fiables en lo que respecta a la puntualidad. Esto depende de muchos factores, como el origen y destino, duración del vuelo, hora del día, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agrupación y agregaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p><b>PREGUNTA</b>: ¿Cuáles son los vuelos (origen, destino) con mayor retraso medio? ¿Cuántos vuelos existen entre cada par de aeropuertos?</p>\n",
    "<p><b>PISTA</b>: Tras hacer las agregaciones para cada pareja \"Origin\", \"Dest\" (una agregación para el retraso medio y otra para contar), aplica el método sort(F.col(\"avgDelay\").desc()) para ordenar de forma decreciente por la nueva columna del retraso medio.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------------------+---------+\n",
      "|Origin|Dest|      retrasoMedio|numVuelos|\n",
      "+------+----+------------------+---------+\n",
      "|   RDM| MFR|            1347.0|        2|\n",
      "|   MDT| HPN|             798.0|        1|\n",
      "|   ORD| GTF|             212.0|        1|\n",
      "|   ICT| DAY|             210.0|        1|\n",
      "|   ELM| ATL|             169.0|        2|\n",
      "|   DSM| PIA|             168.0|        1|\n",
      "|   MSP| LEX|             153.0|        2|\n",
      "|   PPG| HNL|144.22222222222223|       18|\n",
      "|   HNL| PPG|143.05555555555554|       18|\n",
      "|   YNG| PIE|             141.0|        1|\n",
      "|   BUF| MSP|             128.0|        2|\n",
      "|   CMH| HOU|             120.0|        1|\n",
      "|   HRL| DAL|             111.0|        1|\n",
      "|   PIE| YNG|             104.0|        1|\n",
      "|   AVP| SFB|              93.0|        1|\n",
      "|   DFW| LIH| 91.71428571428571|       15|\n",
      "|   OWB| SFB|           90.5625|       19|\n",
      "|   CPR| LAS|              85.0|        1|\n",
      "|   TWF| ORD|              83.0|        1|\n",
      "|   LAS| CPR|              82.0|        1|\n",
      "+------+----+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(cleanFlightsDF.select(F.col(\"Origin\"),F.col(\"Dest\"),F.col(\"ArrDelay\"))\\\n",
    "                       .groupBy(F.col(\"Origin\"),F.col(\"Dest\"))\\\n",
    "                       .agg(avg(F.col(\"ArrDelay\")).alias(\"retrasoMedio\"),count(\"*\").alias(\"numVuelos\"))\n",
    "                       .sort(F.col(\"retrasoMedio\").desc())\n",
    "                       .show(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- FlightDate: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- OriginCity: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- DestCity: string (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- DepDelay: double (nullable = true)\n",
      " |-- ArrTime: integer (nullable = true)\n",
      " |-- ArrDelay: double (nullable = true)\n",
      " |-- Cancelled: double (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: double (nullable = true)\n",
      " |-- ActualElapsedTime: double (nullable = true)\n",
      " |-- AirTime: double (nullable = true)\n",
      " |-- Distance: double (nullable = true)\n",
      " |-- CarrierDelay: double (nullable = true)\n",
      " |-- WeatherDelay: double (nullable = true)\n",
      " |-- NASDelay: double (nullable = true)\n",
      " |-- SecurityDelay: double (nullable = true)\n",
      " |-- LateAircraftDelay: double (nullable = true)\n",
      " |-- ArrDelayCat: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanFlightsDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p><b>PREGUNTA</b>: ¿Es el avión un medio de transporte fiable? Mostrar el número de vuelos en cada categoría de retraso.</p>\n",
    "En lugar de llamar agg(F.count(\"*\")), podemos llamar a la transformación count() sobre el resultado de groupBy(), y creará\n",
    "automáticamente una columna llamada \"count\" con los conteos para cada grupo.\n",
    "<p> Ahora agrupar también por cada aeropuerto de origen, y mostrando una columna distinta por cada tipo de retraso, con el recuento. PISTA: utilizar la función pivot(\"colName\").</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+----------------+-----------+----------------+-------------+------------------+\n",
      "|Origin|Huge_Conteo|Huge_maxArrDelay|None_Conteo|None_maxArrDelay|Slight_Conteo|Slight_maxArrDelay|\n",
      "+------+-----------+----------------+-----------+----------------+-------------+------------------+\n",
      "|   ABE|         97|           674.0|        554|            14.0|          112|              59.0|\n",
      "|   ABI|         37|           397.0|        229|            14.0|           46|              59.0|\n",
      "|   ABQ|        242|          1073.0|       3368|            14.0|          434|              59.0|\n",
      "|   ABR|         16|           584.0|         95|            13.0|            7|              33.0|\n",
      "|   ABY|         20|           641.0|        117|            14.0|           23|              48.0|\n",
      "|   ACT|         23|           424.0|        170|            14.0|           22|              59.0|\n",
      "|   ACV|         18|           255.0|        147|            14.0|           16|              59.0|\n",
      "|   ACY|         32|           589.0|        514|            14.0|           44|              58.0|\n",
      "|   ADK|          2|            null|         12|             9.0|            3|              48.0|\n",
      "|   ADQ|          9|           120.0|         81|            13.0|           10|              55.0|\n",
      "|   AEX|         50|           467.0|        397|            14.0|           73|              56.0|\n",
      "|   AGS|         78|          1366.0|        603|            14.0|          104|              58.0|\n",
      "|   ALB|        276|           569.0|       1939|            14.0|          361|              59.0|\n",
      "|   ALO|         14|           316.0|         78|            11.0|           18|              56.0|\n",
      "|   ALW|          8|           131.0|        144|            13.0|            8|              55.0|\n",
      "|   AMA|         70|           518.0|        728|            14.0|           78|              59.0|\n",
      "|   ANC|        167|          1064.0|       2322|            14.0|          223|              59.0|\n",
      "|   APN|         20|           469.0|         67|            12.0|           13|              47.0|\n",
      "|   ART|         41|          1149.0|         70|            10.0|            7|              57.0|\n",
      "|   ASE|        414|          1383.0|       1261|            14.0|          216|              59.0|\n",
      "+------+-----------+----------------+-----------+----------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UTilizamos pivot para coger todos los valores posibles de \"ArrDelayCat\".\n",
    "# Cada posible valor se transforma en una columna\n",
    "# Le agregamos un conteo para cada tipo y obtenemos el máximo para cada tipo también\n",
    "# Después lo ordenamos por origen (por ningún motivo real)\n",
    "\n",
    "pivot = cleanFlightsDF.groupBy(\"Origin\")\\\n",
    "                      .pivot(\"ArrDelayCat\").agg(\n",
    "                          F.count(\"*\").alias(\"Conteo\"),\n",
    "                          F.max(\"ArrDelay\").alias(\"maxArrDelay\"))\\\n",
    "                        .sort(\"Origin\")\n",
    "pivot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<p><b>PREGUNTA</b>: ¿Hay relación entre el día de la semana y el retraso a la salida o a la llegada?</p>\n",
    "    <p><b>PISTA</b>: Calcula el retraso medio a la salida y a la llegada para cada día de la semana y ordena por una de ellas descendentemente.</p>\n",
    "    <p> Ahora haz lo mismo para cada día pero solo con el retraso a la llegada, desagregado por cada aeropuerto de salida, utilizando la función pivot(). </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+------------------+\n",
      "|DayOfWeek|       avg(ArrDelay)|     avg(DepDelay)|\n",
      "+---------+--------------------+------------------+\n",
      "|        1|   5.862438897075216| 11.32756264236902|\n",
      "|        2|   2.135074656089688| 8.522952978196345|\n",
      "|        3|   3.218429729970591| 9.017015081296748|\n",
      "|        4|   2.091846182334424|  8.39038196495373|\n",
      "|        5|    5.97030310020579|11.769760656650647|\n",
      "|        6|-0.09118143102478996|  7.10215602989408|\n",
      "|        7|   5.152362335166655|11.203569495597534|\n",
      "+---------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(cleanFlightsDF\n",
    " .groupBy(F.col(\"DayOfWeek\"))\\\n",
    " .agg(F.mean(\"ArrDelay\"),F.mean(\"DepDelay\"))\\\n",
    " .sort(\"DayOfWeek\")\\\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Origin</th>\n",
       "      <th>1_avg(ArrDelay)</th>\n",
       "      <th>1_avg(DepDelay)</th>\n",
       "      <th>2_avg(ArrDelay)</th>\n",
       "      <th>2_avg(DepDelay)</th>\n",
       "      <th>3_avg(ArrDelay)</th>\n",
       "      <th>3_avg(DepDelay)</th>\n",
       "      <th>4_avg(ArrDelay)</th>\n",
       "      <th>4_avg(DepDelay)</th>\n",
       "      <th>5_avg(ArrDelay)</th>\n",
       "      <th>5_avg(DepDelay)</th>\n",
       "      <th>6_avg(ArrDelay)</th>\n",
       "      <th>6_avg(DepDelay)</th>\n",
       "      <th>7_avg(ArrDelay)</th>\n",
       "      <th>7_avg(DepDelay)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABE</td>\n",
       "      <td>19.512397</td>\n",
       "      <td>19.537190</td>\n",
       "      <td>22.776699</td>\n",
       "      <td>24.019417</td>\n",
       "      <td>15.256410</td>\n",
       "      <td>16.119658</td>\n",
       "      <td>8.037037</td>\n",
       "      <td>12.555556</td>\n",
       "      <td>16.581818</td>\n",
       "      <td>21.890909</td>\n",
       "      <td>8.360465</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>21.042105</td>\n",
       "      <td>20.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABI</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>22.960784</td>\n",
       "      <td>27.727273</td>\n",
       "      <td>26.272727</td>\n",
       "      <td>7.325000</td>\n",
       "      <td>6.150000</td>\n",
       "      <td>2.363636</td>\n",
       "      <td>4.409091</td>\n",
       "      <td>8.111111</td>\n",
       "      <td>8.177778</td>\n",
       "      <td>8.033333</td>\n",
       "      <td>8.466667</td>\n",
       "      <td>7.444444</td>\n",
       "      <td>9.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABQ</td>\n",
       "      <td>2.609034</td>\n",
       "      <td>10.524922</td>\n",
       "      <td>0.129508</td>\n",
       "      <td>7.241042</td>\n",
       "      <td>-0.380719</td>\n",
       "      <td>6.859477</td>\n",
       "      <td>0.090753</td>\n",
       "      <td>6.832765</td>\n",
       "      <td>-1.865854</td>\n",
       "      <td>5.296167</td>\n",
       "      <td>-4.571429</td>\n",
       "      <td>4.402715</td>\n",
       "      <td>-2.176806</td>\n",
       "      <td>6.283554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABR</td>\n",
       "      <td>17.411765</td>\n",
       "      <td>17.529412</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>11.375000</td>\n",
       "      <td>14.111111</td>\n",
       "      <td>21.555556</td>\n",
       "      <td>38.562500</td>\n",
       "      <td>39.812500</td>\n",
       "      <td>25.750000</td>\n",
       "      <td>32.562500</td>\n",
       "      <td>10.933333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>11.875000</td>\n",
       "      <td>21.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABY</td>\n",
       "      <td>11.923077</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>3.434783</td>\n",
       "      <td>19.272727</td>\n",
       "      <td>13.173913</td>\n",
       "      <td>18.826087</td>\n",
       "      <td>15.545455</td>\n",
       "      <td>-0.826087</td>\n",
       "      <td>-3.136364</td>\n",
       "      <td>30.600000</td>\n",
       "      <td>28.750000</td>\n",
       "      <td>49.350000</td>\n",
       "      <td>51.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>XNA</td>\n",
       "      <td>11.771341</td>\n",
       "      <td>15.024169</td>\n",
       "      <td>12.138037</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>10.859281</td>\n",
       "      <td>13.627976</td>\n",
       "      <td>2.974277</td>\n",
       "      <td>7.535256</td>\n",
       "      <td>8.895425</td>\n",
       "      <td>12.824104</td>\n",
       "      <td>3.131579</td>\n",
       "      <td>5.598684</td>\n",
       "      <td>9.015152</td>\n",
       "      <td>8.754717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>YAK</td>\n",
       "      <td>-16.846154</td>\n",
       "      <td>-13.500000</td>\n",
       "      <td>-16.333333</td>\n",
       "      <td>-19.500000</td>\n",
       "      <td>-17.444444</td>\n",
       "      <td>-17.111111</td>\n",
       "      <td>-9.600000</td>\n",
       "      <td>-17.066667</td>\n",
       "      <td>-21.187500</td>\n",
       "      <td>-21.187500</td>\n",
       "      <td>-21.000000</td>\n",
       "      <td>-21.812500</td>\n",
       "      <td>-13.666667</td>\n",
       "      <td>-16.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>YKM</td>\n",
       "      <td>-1.068966</td>\n",
       "      <td>-0.758621</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-1.600000</td>\n",
       "      <td>10.812500</td>\n",
       "      <td>11.593750</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>2.592593</td>\n",
       "      <td>-1.222222</td>\n",
       "      <td>6.125000</td>\n",
       "      <td>6.583333</td>\n",
       "      <td>3.185185</td>\n",
       "      <td>3.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>YNG</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>YUM</td>\n",
       "      <td>-11.027778</td>\n",
       "      <td>-0.444444</td>\n",
       "      <td>-13.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>11.444444</td>\n",
       "      <td>-3.225806</td>\n",
       "      <td>6.193548</td>\n",
       "      <td>-10.218750</td>\n",
       "      <td>3.062500</td>\n",
       "      <td>-13.636364</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-12.875000</td>\n",
       "      <td>-0.218750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Origin  1_avg(ArrDelay)  1_avg(DepDelay)  2_avg(ArrDelay)  \\\n",
       "0      ABE        19.512397        19.537190        22.776699   \n",
       "1      ABI        21.000000        22.960784        27.727273   \n",
       "2      ABQ         2.609034        10.524922         0.129508   \n",
       "3      ABR        17.411765        17.529412        10.000000   \n",
       "4      ABY        11.923077        15.000000         0.041667   \n",
       "..     ...              ...              ...              ...   \n",
       "348    XNA        11.771341        15.024169        12.138037   \n",
       "349    YAK       -16.846154       -13.500000       -16.333333   \n",
       "350    YKM        -1.068966        -0.758621         0.166667   \n",
       "351    YNG         9.000000        15.000000              NaN   \n",
       "352    YUM       -11.027778        -0.444444       -13.166667   \n",
       "\n",
       "     2_avg(DepDelay)  3_avg(ArrDelay)  3_avg(DepDelay)  4_avg(ArrDelay)  \\\n",
       "0          24.019417        15.256410        16.119658         8.037037   \n",
       "1          26.272727         7.325000         6.150000         2.363636   \n",
       "2           7.241042        -0.380719         6.859477         0.090753   \n",
       "3          11.375000        14.111111        21.555556        38.562500   \n",
       "4           3.434783        19.272727        13.173913        18.826087   \n",
       "..               ...              ...              ...              ...   \n",
       "348        19.600000        10.859281        13.627976         2.974277   \n",
       "349       -19.500000       -17.444444       -17.111111        -9.600000   \n",
       "350        -1.600000        10.812500        11.593750         0.678571   \n",
       "351              NaN              NaN              NaN       141.000000   \n",
       "352         0.333333         1.416667        11.444444        -3.225806   \n",
       "\n",
       "     4_avg(DepDelay)  5_avg(ArrDelay)  5_avg(DepDelay)  6_avg(ArrDelay)  \\\n",
       "0          12.555556        16.581818        21.890909         8.360465   \n",
       "1           4.409091         8.111111         8.177778         8.033333   \n",
       "2           6.832765        -1.865854         5.296167        -4.571429   \n",
       "3          39.812500        25.750000        32.562500        10.933333   \n",
       "4          15.545455        -0.826087        -3.136364        30.600000   \n",
       "..               ...              ...              ...              ...   \n",
       "348         7.535256         8.895425        12.824104         3.131579   \n",
       "349       -17.066667       -21.187500       -21.187500       -21.000000   \n",
       "350        -1.250000         2.592593        -1.222222         6.125000   \n",
       "351       111.000000              NaN              NaN              NaN   \n",
       "352         6.193548       -10.218750         3.062500       -13.636364   \n",
       "\n",
       "     6_avg(DepDelay)  7_avg(ArrDelay)  7_avg(DepDelay)  \n",
       "0           9.500000        21.042105        20.656250  \n",
       "1           8.466667         7.444444         9.800000  \n",
       "2           4.402715        -2.176806         6.283554  \n",
       "3           8.333333        11.875000        21.125000  \n",
       "4          28.750000        49.350000        51.947368  \n",
       "..               ...              ...              ...  \n",
       "348         5.598684         9.015152         8.754717  \n",
       "349       -21.812500       -13.666667       -16.866667  \n",
       "350         6.583333         3.185185         3.296296  \n",
       "351              NaN              NaN              NaN  \n",
       "352        -0.333333       -12.875000        -0.218750  \n",
       "\n",
       "[353 rows x 15 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas = (cleanFlightsDF.groupBy(F.col(\"Origin\"))\\\n",
    " .pivot(\"DayOfWeek\")\\\n",
    " .agg(F.mean(\"ArrDelay\"),F.mean(\"DepDelay\"))\\\n",
    " .sort(\"Origin\")\\\n",
    " .toPandas())\n",
    "\n",
    "pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p><b>LA FUNCIÓN PIVOT</b>: Puede ser interesante ver, para cada (Origin, Dest), el retraso promedio por\n",
    "día de la semana. Si agrupamos por esas tres variables (Origin, Dest, DayOfWeek), nuestro resultado tendría demasiadas filas para ser fácil de visualizar (7 x 1009 ya que hay 1009 combinaciones de (Origin, DayOfWeek)). En cambio, vamos a crear 7 columnas, una por día de la semana, en nuestro resultado DF. Lo haremos utilizando una de las variables de agrupación (DayOfWeek) como <i> variable pivot</i>. Como esta variable tiene 7 valores distintos, se crearán 7 columnas nuevas. De esta manera, visualizaremos toda la información de cada combinación (Origen, Dest) condensada en una fila con 7 columnas con los 7 retrasos promedio correspondientes a ese (Origen, Dest) en cada día de la semana.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones JOIN y de ventana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estaría bien tener el retraso promedio de una ruta junto a cada vuelo, para que podamos ver qué vuelos tuvieron un retraso que fue superior o inferior al retraso promedio de esa ruta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b> PREGUNTA </b>:\n",
    "Usa el averageDelayOriginDestDF creado anteriormente, elimina la columna de conteo y luego únerlo con cleanFlightsDF, utilizando Origin y Dest como columnas de enlace. Finalmente, selecciona solo las columnas Origin, Dest, DayOfWeek, ArrDelay y avgDelay del resultado.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageDelayOriginDestDF = cleanFlightsDF.groupBy(\"Origin\", \"Dest\").agg(\n",
    "    F.mean(\"ArrDelay\").alias(\"avgArrDelay\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'averageDelayOriginDestDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RAFAEL~1.GOM\\AppData\\Local\\Temp/ipykernel_13952/1128754060.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m preguntaFour = averageDelayOriginDestDF.join(cleanFlightsDF,\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maverageDelayOriginDestDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Origin\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcleanFlightsDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Origin\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m (averageDelayOriginDestDF[\"Dest\"] == cleanFlightsDF[\"Dest\"]))\n\u001b[0;32m      5\u001b[0m preguntaFour.select(averageDelayOriginDestDF[\"Origin\"], \\\n",
      "\u001b[1;31mNameError\u001b[0m: name 'averageDelayOriginDestDF' is not defined"
     ]
    }
   ],
   "source": [
    "preguntaFour = averageDelayOriginDestDF.join(cleanFlightsDF,\n",
    "on =\n",
    "(averageDelayOriginDestDF[\"Origin\"] == cleanFlightsDF[\"Origin\"]) &\\\n",
    "(averageDelayOriginDestDF[\"Dest\"] == cleanFlightsDF[\"Dest\"]))\n",
    "preguntaFour.select(averageDelayOriginDestDF[\"Origin\"], \\\n",
    "averageDelayOriginDestDF[\"Dest\"], \"DayOfWeek\", \"ArrDelay\", \"avgArrDelay\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <p><b>BONUS (OPCIONAL)</b>: crear una nueva columna <i>belowAverage</i> que tenga valor True si ArrDelay es menor que el avgDelay de esa ruta, y False en caso contrario. No utilizar la función when() sino el operador de comparación directamente entre columnas, la cual devolverá una columna booleana.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREGUNTA**: repetir la operación utilizando funciones de ventana, sin usar `join`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> PREGUNTA </b>: Vamos a construir otro DF con información sobre los aeropuertos (en una situación real, tendríamos otra tabla en la base de datos como la tabla de la entidad Aeropuerto). Sin embargo, solo tenemos información sobre algunos aeropuertos. Nos gustaría agregar esta información a cleanFlightsDF como nuevas columnas, teniendo en cuenta que queremos que la información del aeropuerto coincida con el aeropuerto de origen de flightsDF. Utilizar la operación de unión adecuada para asegurarse de que no se perderá ninguna de las filas existentes de cleanFlightsDF después de la unión.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportsDF = spark.createDataFrame([\n",
    "    (\"JFK\", \"John F. Kennedy International Airport\", 1948),\n",
    "    (\"LIT\", \"Little Rock National Airport\", 1931),\n",
    "    (\"SEA\", \"Seattle-Tacoma International Airport\", 1949),\n",
    "], [\"IATA\", \"FullName\", \"Year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+------------------------------------+----+\n",
      "|Origin|Dest|IATA|FullName                            |Year|\n",
      "+------+----+----+------------------------------------+----+\n",
      "|SEA   |JFK |SEA |Seattle-Tacoma International Airport|1949|\n",
      "|SEA   |LGB |SEA |Seattle-Tacoma International Airport|1949|\n",
      "|SEA   |BOS |SEA |Seattle-Tacoma International Airport|1949|\n",
      "|SEA   |BOS |SEA |Seattle-Tacoma International Airport|1949|\n",
      "|SEA   |LGB |SEA |Seattle-Tacoma International Airport|1949|\n",
      "|SEA   |JFK |SEA |Seattle-Tacoma International Airport|1949|\n",
      "|SEA   |LGB |SEA |Seattle-Tacoma International Airport|1949|\n",
      "|SEA   |BOS |SEA |Seattle-Tacoma International Airport|1949|\n",
      "|SEA   |BOS |SEA |Seattle-Tacoma International Airport|1949|\n",
      "|SEA   |LGB |SEA |Seattle-Tacoma International Airport|1949|\n",
      "+------+----+----+------------------------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedFlightsDF = cleanFlightsDF.join(airportsDF,\n",
    "                                      on = cleanFlightsDF[\"Origin\"] == airportsDF[\"IATA\"],\n",
    "                                      how = \"left_outer\")\n",
    "\n",
    "# PREGUNTA: mostrar algunas filas donde FullName no sea null\n",
    "joinedFlightsDF.filter(~(F.col(\"FullName\").isNull()))\\\n",
    "                .select(\"Origin\",\"Dest\",\"IATA\",\"FullName\",\"Year\")\\\n",
    "                .show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined functions (UDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a construir un UDF para convertir millas a kilómetros. Ten en cuenta que esto podría hacerse fácilmente multiplicando directamente la columna de millas por 1.6 (y sería mucho más eficiente), ya que Spark permite el producto entre una columna y un número. En todos los casos en los que Spark proporciona funciones integradas para realizar una tarea (como esta), debes usar esas funciones y no una UDF. Las UDF deben emplearse solo cuando no hay otra opción.\n",
    "\n",
    "La razón es que las funciones integradas de Spark están optimizadas y Catalyst, el optimizador automático de código integrado en Spark, puede optimizarlo aún más. Sin embargo, las UDF son una caja negra para Catalyst y su contenido no se optimizará, y por lo tanto, generalmente son mucho más lentas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 450, in mapper\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 450, in <genexpr>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 85, in <lambda>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\RAFAEL~1.GOM\\AppData\\Local\\Temp/ipykernel_13952/312643482.py\", line 6, in milesToKm\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'float'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RAFAEL~1.GOM\\AppData\\Local\\Temp/ipykernel_13952/312643482.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mflightsWithKm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleanFlightsDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DistKm\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mudfMilesToKm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Distance\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mflightsWithKm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Origin\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Dest\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Distance\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DistKM\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m              \u001b[1;33m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m              \u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 450, in mapper\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 450, in <genexpr>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 85, in <lambda>\n  File \"C:\\spark\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Users\\RAFAEL~1.GOM\\AppData\\Local\\Temp/ipykernel_13952/312643482.py\", line 6, in milesToKm\nTypeError: unsupported operand type(s) for *: 'NoneType' and 'float'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Primer paso: crear una función de Python que reciba UN número y lo multiplique por 1.6\n",
    "def milesToKm(miles):\n",
    "    return miles*1.6\n",
    "\n",
    "# Vamos a probarla\n",
    "print(milesToKm(5)) # 5 millas a km: 8 km\n",
    "\n",
    "# Segundo paso: crear un objeto UDF que envuelva a nuestra función. \n",
    "# Hay que especificar el tipo de dato que devuelve nuestra función\n",
    "udfMilesToKm = F.udf(milesToKm, DoubleType())\n",
    "\n",
    "# Con esto, Spark será capaz de llamar a nuestra función milesToKm sobre cada uno de los valores de una columna numérica.\n",
    "# Spark enviará el código de nuestra función a los executors a través de la red, y cada executor la ejecutará sobre las\n",
    "# particiones (una por una) que estén en ese executor\n",
    "\n",
    "# Tercer paso: vamos a probar la UDF añadiendo una nueva columna con el resultado de la conversión\n",
    "flightsWithKm = cleanFlightsDF.withColumn(\"DistKm\", udfMilesToKm(F.col(\"Distance\")))\n",
    "\n",
    "flightsWithKm.select(\"Origin\", \"Dest\", \"Distance\", \"DistKM\")\\\n",
    "             .distinct()\\\n",
    "             .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<p><b>BONUS</b>: Crea tu propia UDF que convierta DayOfWeek en una cadena.\n",
    "Puedes hacerlo creando una función de Python que reciba un número entero y devuelva el día de la semana,\n",
    "simplemente leyendo desde un vector de cadenas de longitud 7 el valor en la posición indicada por el argumento entero. Para la UDF, recuerda que tu función devuelve un StringType(). Finalmente, prueba tu UDF creando una nueva columna \"DayOfWeekString\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---------+---------------+\n",
      "|Origin|Dest|DayOfWeek|DayOfWeekString|\n",
      "+------+----+---------+---------------+\n",
      "|   SEA| JFK|        2|        Tuesday|\n",
      "|   DFW| FLL|        7|         Sunday|\n",
      "|   PHL| SJU|        1|         Monday|\n",
      "|   GEG| DFW|        6|       Saturday|\n",
      "|   OAK| PHX|        2|        Tuesday|\n",
      "|   LAX| DEN|        7|         Sunday|\n",
      "|   LAX| SEA|        1|         Monday|\n",
      "|   OAK| PDX|        4|       Thursday|\n",
      "|   LAS| PDX|        5|         Friday|\n",
      "|   SLC| PDX|        5|         Friday|\n",
      "|   SJC| LIH|        6|       Saturday|\n",
      "|   SJC| EWR|        7|         Sunday|\n",
      "|   KOA| ANC|        7|         Sunday|\n",
      "|   SEA| DTW|        1|         Monday|\n",
      "|   DTW| SEA|        1|         Monday|\n",
      "|   SEA| BOS|        3|      Wednesday|\n",
      "|   JFK| LAS|        5|         Friday|\n",
      "|   ATL| XNA|        5|         Friday|\n",
      "|   OAK| SLC|        5|         Friday|\n",
      "|   MSP| ANC|        5|         Friday|\n",
      "+------+----+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Primer paso: creamos una función de python que convierte un número entero en el día de la semana como cadena\n",
    "def dayOfWeekToString(dayInteger):\n",
    "    # En nuestros datos Monday es 1 pero las listas de python empiezan en el 0 y \n",
    "    # queremos usar el dayInteger como índice del vector\n",
    "    daysOfWeek = [\"\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    #return ????\n",
    "    \n",
    "# Segundo paso: ajustamos nuestra función con un Spark UDF para que Spark pueda invocarlo en cada valor de una columna completa\n",
    "# De esta manera, Spark puede enviar nuestra función a los ejecutores, que eventualmente ejecutarán la función en las particiones\n",
    "# de los datos que tiene cada ejecutor\n",
    "dayOfWeekStringUDF = \n",
    "\n",
    "# Tercer paso: intentemos nuestro UDF agregando una nueva columna que resulta de transformar (a través del UDF) el\n",
    "# columna existente DayOfWeek\n",
    "flightsWithDayOfWeekStr = \n",
    "\n",
    "flightsWithDayOfWeekStr.select(\"Origin\", \"Dest\", \"DayOfWeek\", \"DayOfWeekString\")\\\n",
    "                       .distinct()\\\n",
    "                       .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
