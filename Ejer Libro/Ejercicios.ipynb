{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52646ecb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Capítulo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7a767d7-a4c0-4737-94bc-e506cb7304de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4eda30d\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    ".builder()\n",
    ".appName(\"Scala\")\n",
    ".master(\"local\")\n",
    ".enableHiveSupport()\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a3e66-60ff-4fa0-973a-7bdceca97955",
   "metadata": {},
   "source": [
    "    a. Descargar el Quijote (https://gist.github.com/jsdario/6d6c69398cb0c73111e49f1218960f79) y aplicar no solo count (para obtener el número de líneas) y show sino probar distintas sobrecargas del método show (con y sin truncate, indicando/sin indicar num de filas, etc) así como también los métodos head, take, first (diferencias entre estos 3?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a26c4edf-34b5-441c-809a-ffdd0b276355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuijoteDF: org.apache.spark.sql.DataFrame = [value: string]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Se puede leer como CSV pero vamos a leerlo como archivo de texto porque es... un archivo de texto.\n",
    "\n",
    "val QuijoteDF = spark.read.text(\"el_quijote.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8ef948b-f129-4bd4-bf7a-9f53b778a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------+\n",
      "|value                                                                                         |\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "|DON QUIJOTE DE LA MANCHA                                                                      |\n",
      "|Miguel de Cervantes Saavedra                                                                  |\n",
      "|                                                                                              |\n",
      "|PRIMERA PARTE                                                                                 |\n",
      "|CAPI?TULO 1: Que trata de la condicio?n y ejercicio del famoso hidalgo D. Quijote de la Mancha|\n",
      "+----------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "QuijoteDF.show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20214dc4-349d-45eb-bf2b-7fa683989adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 2186\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QuijoteDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31c7f4a3-dfa8-425a-bf92-05468b3811bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Array[org.apache.spark.sql.Row] = Array([DON QUIJOTE DE LA MANCHA])\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QuijoteDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efeecba9-7867-4d12-822e-3225288bae67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Array[org.apache.spark.sql.Row] = Array([DON QUIJOTE DE LA MANCHA], [Miguel de Cervantes Saavedra], [], [PRIMERA PARTE], [CAPI?TULO 1: Que trata de la condicio?n y ejercicio del famoso hidalgo D. Quijote de la Mancha])\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QuijoteDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9320015-b0a2-41fa-9b44-70a9924537f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res8: org.apache.spark.sql.Row = [DON QUIJOTE DE LA MANCHA]\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QuijoteDF.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6100b5-8e64-4874-874b-450d62c27d90",
   "metadata": {},
   "source": [
    "Take tiene que recibir un número de líneas si o si, mientras que head puede no recibir ningún número y devolver la primera línea. First siempre devuelve la primera línea. Además, si head no recibe parámetro devuelve una fila, mientras que take(1) devuelve un array que contiene una fila."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398256e2-d0ba-4019-b71d-7d9853ca364c",
   "metadata": {},
   "source": [
    "    b. Del ejercicio de MnM aplicar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d0b9b1-e1bf-4d53-ab59-752dc8006464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Count|\n",
      "+-----+------+-----+\n",
      "|TX   |Red   |20   |\n",
      "|NV   |Blue  |66   |\n",
      "|CO   |Blue  |79   |\n",
      "|OR   |Blue  |71   |\n",
      "|WA   |Yellow|93   |\n",
      "+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mnmFile: String = C:/Users/rafael.gomez/learning/chapter2/scala/data/mnm_dataset.csv\r\n",
       "mnmDF: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Leo el dataset de MnM\n",
    "\n",
    "// get the M&M data set file name\n",
    "val mnmFile = \"C:/Users/rafael.gomez/learning/chapter2/scala/data/mnm_dataset.csv\"\n",
    "// read the file into a Spark DataFrame\n",
    "val mnmDF = spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(mnmFile)\n",
    "// display DataFrame\n",
    "mnmDF.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4d81f4-4ebb-4dc5-a853-97fe10988752",
   "metadata": {},
   "source": [
    "        i. Otras operaciones de agregación como el Max con otro tipo de ordenamiento (descendiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82a9aa2b-76b9-4c73-b04e-89f6ee033145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "45b10684-63bd-4be9-823b-903530aad002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|State| Color|max(Count)|\n",
      "+-----+------+----------+\n",
      "|   WA|   Red|       100|\n",
      "|   NM| Green|       100|\n",
      "|   NV|   Red|       100|\n",
      "|   NV| Brown|       100|\n",
      "|   CA|  Blue|       100|\n",
      "|   WA|Orange|       100|\n",
      "|   UT|  Blue|       100|\n",
      "|   WY| Green|       100|\n",
      "|   AZ|Orange|       100|\n",
      "|   NV|Orange|       100|\n",
      "+-----+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "caCountMnNDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val caCountMnNDF = mnmDF.select(\"*\")\n",
    "  .groupBy(\"State\", \"Color\")\n",
    "  .max(\"Count\")\n",
    "  .orderBy(desc(\"max(Count)\"))\n",
    "\n",
    "// show the resulting aggregation for California\n",
    "caCountMnNDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b61f328-4bb2-4846-a09c-537a09012845",
   "metadata": {},
   "source": [
    "        ii. hacer un ejercicio como el “where” de CA que aparece en el libro pero indicando más opciones de estados (p.e. NV, TX, CA, CO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d55e974-b84b-49a3-ad96-712e299a9b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|State| Color|sum(Count)|\n",
      "+-----+------+----------+\n",
      "|   CA|Yellow|    100956|\n",
      "|   CA| Brown|     95762|\n",
      "|   NV|Orange|     93929|\n",
      "|   CA| Green|     93505|\n",
      "|   NV| Brown|     92478|\n",
      "|   CA|   Red|     91527|\n",
      "|   NV|Yellow|     91390|\n",
      "|   NV| Green|     91331|\n",
      "|   CA|Orange|     90311|\n",
      "|   NV|  Blue|     90003|\n",
      "+-----+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "caCountMnNDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val caCountMnNDF = mnmDF.select(\"*\")\n",
    "  .where(col(\"State\") === \"CA\" || col(\"State\") === \"NV\")\n",
    "  .groupBy(\"State\", \"Color\")\n",
    "  .sum(\"Count\")\n",
    "  .orderBy(desc(\"sum(Count)\"))\n",
    "\n",
    "// show the resulting aggregation for California\n",
    "caCountMnNDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d28ed51-0da1-405a-b529-6ffc7c479263",
   "metadata": {},
   "source": [
    "        iii. Hacer un ejercicio donde se calculen en una misma operación el Max, Min, Avg, Count. Revisar el API (documentación) donde encontrarán este ejemplo:\n",
    "        ds.agg(max($\"age\"), avg($\"salary\")) ds.groupBy().agg(max($\"age\"), avg($\"salary\"))\n",
    "        NOTA: $ es un alias de col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7740d6e6-73dd-4fa0-ad64-691848448890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------------+\n",
      "|max(Count)|min(Count)|       avg(Count)|count(Count)|\n",
      "+----------+----------+-----------------+------------+\n",
      "|       100|        10|55.00090000900009|       99999|\n",
      "+----------+----------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnmDF.select(max(\"Count\"),min(\"Count\"),avg(\"Count\"),count(\"Count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "21af1ae0-b141-48bb-8a88-f46a7595c00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------------+\n",
      "|max(Count)|min(Count)|       avg(Count)|count(Count)|\n",
      "+----------+----------+-----------------+------------+\n",
      "|       100|        10|55.00090000900009|       99999|\n",
      "+----------+----------+-----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnmDF.agg(max($\"Count\"),min($\"Count\"),avg($\"Count\"),count($\"Count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740e28f8-8733-4c8e-966e-645a7d074fcf",
   "metadata": {},
   "source": [
    "        iv. Hacer también ejercicios en SQL creando tmpView"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fa915b-22f7-4ad3-8cd0-6e2c289b5e64",
   "metadata": {},
   "source": [
    "CreateOrReplaceTempView will create a temporary view of the table on memory it is not persistent at this moment but you can run SQL query on top of that. if you want to save it you can either persist or use saveAsTable to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd3263b2-dc46-4b6a-af25-ad8088ea0ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.createOrReplaceTempView(\"mnmTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4f2dfabb-8f26-4080-a50c-2cec32ad8fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Count|\n",
      "+-----+------+-----+\n",
      "|WA   |Yellow|93   |\n",
      "|CA   |Yellow|53   |\n",
      "|WA   |Yellow|20   |\n",
      "|NM   |Yellow|15   |\n",
      "|WY   |Yellow|48   |\n",
      "+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT * from mnmTemp where State==\"CA\" or Color=\"Yellow\" \"\"\").show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58331d89-79d8-4d52-908d-b6fe191ae474",
   "metadata": {},
   "source": [
    "# Capítulo 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b5f8b4-8845-460b-8bb6-6963f11305f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Realizar todos los ejercicios propuestos de libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a92c821c-4632-4a22-993a-bedd6a5760ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n",
       "fireSchema: org.apache.spark.sql.types.StructType = StructType(StructField(CallNumber,IntegerType,true), StructField(UnitID,StringType,true), StructField(IncidentNumber,IntegerType,true), StructField(CallType,StringType,true), StructField(CallDate,StringType,true), StructField(WatchDate,StringType,true), StructField(CallFinalDisposition,StringType,true), StructField(AvailableDtTm,StringType,true), StructField(Address,StringType,true), StructField(City,StringType,true), StructField(Zipcode,IntegerType,true), StructField(Battalion,StringType,true), StructField(StationArea,StringType,true), StructField(Box,StringType,true), StructField(OriginalPriority,StringType,true), StructField(Priority,StringType,true), StructField(FinalPriority,IntegerType,true), ...\r\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Leer el dataset de fire_ts\n",
    "\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    " StructField(\"UnitID\", StringType, true),\n",
    " StructField(\"IncidentNumber\", IntegerType, true),\n",
    " StructField(\"CallType\", StringType, true),\n",
    " StructField(\"CallDate\", StringType, true),\n",
    " StructField(\"WatchDate\", StringType, true),\n",
    " StructField(\"CallFinalDisposition\", StringType, true),\n",
    " StructField(\"AvailableDtTm\", StringType, true),\n",
    " StructField(\"Address\", StringType, true), \n",
    " StructField(\"City\", StringType, true), \n",
    " StructField(\"Zipcode\", IntegerType, true), \n",
    " StructField(\"Battalion\", StringType, true), \n",
    " StructField(\"StationArea\", StringType, true), \n",
    " StructField(\"Box\", StringType, true), \n",
    " StructField(\"OriginalPriority\", StringType, true), \n",
    " StructField(\"Priority\", StringType, true), \n",
    " StructField(\"FinalPriority\", IntegerType, true), \n",
    " StructField(\"ALSUnit\", BooleanType, true), \n",
    " StructField(\"CallTypeGroup\", StringType, true),\n",
    " StructField(\"NumAlarms\", IntegerType, true),\n",
    " StructField(\"UnitType\", StringType, true),\n",
    " StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    " StructField(\"FirePreventionDistrict\", StringType, true),\n",
    " StructField(\"SupervisorDistrict\", StringType, true),\n",
    " StructField(\"Neighborhood\", StringType, true),\n",
    " StructField(\"Location\", StringType, true),\n",
    " StructField(\"RowID\", StringType, true),\n",
    " StructField(\"Delay\", FloatType, true)))\n",
    "\n",
    "// Read the file using the CSV DataFrameReader\n",
    "val sfFireFile=\"C:/Users/rafael.gomez/learning/chapter3/data/sf-fire-calls.csv\"\n",
    "val fireDF = spark.read.schema(fireSchema)\n",
    " .option(\"header\", \"true\")\n",
    " .csv(sfFireFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "87a5e10c-b8db-4d64-a328-38ce07579f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newFireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newFireDF = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "newFireDF\n",
    " .select(\"ResponseDelayedinMins\")\n",
    " .where($\"ResponseDelayedinMins\" > 5)\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "19f87728-7a33-473d-988f-658f85ffe14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fireTsDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Transforma las fechas de fire_DF de String a fecha. (y renombra las columnas)\n",
    "\n",
    "val fireTsDF = newFireDF\n",
    " .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"CallDate\")\n",
    " .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"WatchDate\")\n",
    " .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    " \"MM/dd/yyyy hh:mm:ss a\"))\n",
    " .drop(\"AvailableDtTm\")\n",
    "// Select the converted columns\n",
    "fireTsDF\n",
    " .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    " .show(5, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "177cfdf6-226b-4973-8e39-0a45139616b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Ahora que tenemos la fecha en un formado reconocido por Spark podemos acceder directamente al día, año o mes.\n",
    "\n",
    "fireTsDF\n",
    " .select(year($\"IncidentDate\"))\n",
    " .distinct()\n",
    " .orderBy(year($\"IncidentDate\"))\n",
    " .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6f6ad940-4f11-4d13-b4f4-f7112d279201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            CallType|year(IncidentDate)|\n",
      "+--------------------+------------------+\n",
      "|   Electrical Hazard|              2018|\n",
      "|          Fuel Spill|              2018|\n",
      "|        Water Rescue|              2018|\n",
      "|    Medical Incident|              2018|\n",
      "|        Vehicle Fire|              2018|\n",
      "|  Suspicious Package|              2018|\n",
      "|               Other|              2018|\n",
      "|              HazMat|              2018|\n",
      "|Gas Leak (Natural...|              2018|\n",
      "|       Assist Police|              2018|\n",
      "|Smoke Investigati...|              2018|\n",
      "|Elevator / Escala...|              2018|\n",
      "|Odor (Strange / U...|              2018|\n",
      "|      Structure Fire|              2018|\n",
      "|              Alarms|              2018|\n",
      "|   Traffic Collision|              2018|\n",
      "|Citizen Assist / ...|              2018|\n",
      "|Train / Rail Inci...|              2018|\n",
      "|        Outside Fire|              2018|\n",
      "|           Explosion|              2018|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// What where all the different types of fire calls in 2018?\n",
    "(fireTsDF.select($\"CallType\",year($\"IncidentDate\"))\n",
    ".distinct()\n",
    ".where(year($\"IncidentDate\") === 2018)\n",
    ".show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "73de4a63-2f41-43c7-8f3d-8f68de8742f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|month(IncidentDate)|count|\n",
      "+-------------------+-----+\n",
      "|10                 |1068 |\n",
      "|5                  |1047 |\n",
      "|3                  |1029 |\n",
      "|8                  |1021 |\n",
      "|1                  |1007 |\n",
      "+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// What months within the year 2018 saw the highest number of fire calls?\n",
    "\n",
    "(fireTsDF\n",
    " .where(year($\"IncidentDate\") === 2018)\n",
    " .groupBy(month($\"IncidentDate\"))\n",
    " .count()\n",
    " .orderBy(desc(\"count\"))\n",
    " .show(5,false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d67ff946-9f70-47f4-b7b6-12732b921f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        Neighborhood|count|\n",
      "+--------------------+-----+\n",
      "|          Tenderloin| 1393|\n",
      "|     South of Market| 1052|\n",
      "|             Mission|  911|\n",
      "|Financial Distric...|  764|\n",
      "|Bayview Hunters P...|  513|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Which neighborhood in San Francisco generated the most fire calls in 2018?\n",
    "\n",
    "(fireTsDF\n",
    " .select($\"Neighborhood\")\n",
    " .where($\"City\" === \"San Francisco\" && year($\"IncidentDate\") === 2018)\n",
    " .groupBy($\"Neighborhood\")\n",
    " .count()\n",
    " .orderBy(desc(\"count\"))\n",
    " .show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0dfa7cee-7881-4206-8571-bde45944be8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+\n",
      "|        Neighborhood|   promedioTiempo|\n",
      "+--------------------+-----------------+\n",
      "|           Chinatown|6.190314101143033|\n",
      "|            Presidio|5.829227011272873|\n",
      "|     Treasure Island|5.453703684111436|\n",
      "|        McLaren Park| 4.74404764175415|\n",
      "|Bayview Hunters P...|4.620561962212182|\n",
      "+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Which neighborhoods had the worst response times to fire calls in 2018?\n",
    "\n",
    "// No sé si pide qué barrios tienen peores tiempos así en general o cuales tienen el peor tiempo absoluto\n",
    "// Así que cojo el peor tiempo promedio.\n",
    "\n",
    "(fireTsDF\n",
    " .select($\"Neighborhood\",$\"ResponseDelayedinMins\")\n",
    " .where(year($\"IncidentDate\") === 2018)\n",
    " .groupBy($\"Neighborhood\")\n",
    " .agg(avg($\"ResponseDelayedinMins\").alias(\"promedioTiempo\"))\n",
    " .orderBy(desc(\"promedioTiempo\"))\n",
    " .show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "217a7df9-0d72-436c-adad-cdde7e0576f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Semana|count|\n",
      "+------+-----+\n",
      "|    22|  259|\n",
      "|    40|  255|\n",
      "|    43|  250|\n",
      "|    25|  249|\n",
      "|     1|  246|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Which week in the year in 2018 had the most fire calls?\n",
    "\n",
    "(fireTsDF\n",
    " .select(weekofyear($\"IncidentDate\").alias(\"Semana\"))\n",
    " .where(year($\"IncidentDate\") === 2018)\n",
    " .groupBy($\"Semana\")\n",
    " .count()\n",
    " .orderBy(desc(\"count\"))\n",
    " .show(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb08dc-3142-4133-94ca-70b354535ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Is there a correlation between neighborhood, zip code, and number of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c875f4-7afc-4923-955f-cf3913d20b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "// How can we use Parquet files or SQL tables to store this data and read it back?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bdc233-6947-4658-94cf-cc921ce0fe40",
   "metadata": {},
   "source": [
    "## Leer el CSV del ejemplo del cap2 y obtener la estructura del schema dado por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "23872896-4342-4330-b30c-624d4e30a3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Imagino que el CSV del que estamos hablando es el de los MnM\n",
    "\n",
    "mnmDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e3408a-b853-4f21-a7fe-2184e2c0ed1d",
   "metadata": {},
   "source": [
    "## Cuando se define un schema al definir un campo por ejemplo StructField('Delay', FloatType(), True) ¿qué significa el último parámetro Boolean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1f5b6b-ba56-491f-a2b6-364e935d9c62",
   "metadata": {},
   "source": [
    "Indica si el campo puede ser nulo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494ce8ee-47fb-4ffe-9670-60ece0be746c",
   "metadata": {},
   "source": [
    "## Dataset vs DataFrame (Scala). ¿En qué se diferencian a nivel de código?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a70d3-3f8c-410e-96d3-151f2387784f",
   "metadata": {},
   "source": [
    "En el JVM, las filas en un dataframe no tienen tipo pero en un dataset tienen un tipo estático."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7e269-b62f-4e36-9fb2-ece8f601218f",
   "metadata": {},
   "source": [
    "## Utilizando el mismo ejemplo utilizado en el capítulo para guardar en parquet y guardar los datos en los formatos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382b5baf-0080-4663-ae57-61f56477aa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "// El código en cuestión para guardar en parquet\n",
    "\n",
    "mnmDF.write.format(\"parquet\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"compression\", \"snappy\")\n",
    " .save(\"/tmp/data/parquet/df_parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693db51c-202c-48b8-a770-c64308370314",
   "metadata": {
    "tags": []
   },
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "439cb7b4-a44a-403d-af49-d40adc6087f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.write.format(\"json\")\n",
    " .mode(\"overwrite\")\n",
    " .save(\"/tmp/data/json/df_json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4792e3a1-845b-48b6-9d02-0466cf47f392",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8b6102-0bf8-4a10-96cc-0426fef286db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.write.format(\"csv\").mode(\"overwrite\").save(\"/tmp/data/csv/df_csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1e476d-2489-4939-92f1-9915c52ed496",
   "metadata": {},
   "source": [
    "### AVRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949a9c71-e1e6-4647-a035-1cb7a2e5c5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnmDF.write\n",
    " .format(\"avro\")\n",
    " .mode(\"overwrite\")\n",
    " .save(\"/tmp/data/avro/df_avro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b1b6e5-3ea1-4964-83c8-9607774ca0cd",
   "metadata": {},
   "source": [
    "# Capítulo 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf3665d-15d0-4578-80b0-0a8a5963da88",
   "metadata": {},
   "source": [
    "## GlobalTempView vs TempView"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f3abb-2897-4481-84a8-ec73c583911b",
   "metadata": {},
   "source": [
    "EL tiempo de vida de una TempView está asociado al tiempo de vida de la sparkSession actual, mientras que la de GlobalTempView está asociada a la propia aplicación de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3d3d5-64be-445a-84cb-7bff7479321a",
   "metadata": {},
   "source": [
    "## Leer los AVRO, Parquet, JSON y CSV escritos en el cap3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07957c93-4ab0-4c5f-8a6c-9a6c7abc166e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State| Color|Count|\n",
      "+-----+------+-----+\n",
      "|   TX|   Red|   20|\n",
      "|   NV|  Blue|   66|\n",
      "|   CO|  Blue|   79|\n",
      "|   OR|  Blue|   71|\n",
      "|   WA|Yellow|   93|\n",
      "+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: String = /tmp/data/parquet/df_parquet\r\n",
       "mnmdfparq: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Parquet\n",
    "\n",
    "val file = \"\"\"/tmp/data/parquet/df_parquet\"\"\"\n",
    "val mnmdfparq = spark.read.format(\"parquet\").load(file)\n",
    "\n",
    "mnmdfparq.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df56141-dd88-4f4d-8958-6de192fdd111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+\n",
      "| Color|Count|State|\n",
      "+------+-----+-----+\n",
      "|   Red|   20|   TX|\n",
      "|  Blue|   66|   NV|\n",
      "|  Blue|   79|   CO|\n",
      "|  Blue|   71|   OR|\n",
      "|Yellow|   93|   WA|\n",
      "+------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: String = /tmp/data/json/df_json*\r\n",
       "mnmdfjson: org.apache.spark.sql.DataFrame = [Color: string, Count: bigint ... 1 more field]\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Json\n",
    "\n",
    "val file = \"/tmp/data/json/df_json*\"\n",
    "val mnmdfjson = spark.read.format(\"json\").load(file)\n",
    "\n",
    "mnmdfjson.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ad19158-bd5a-46f4-b645-6c052778b45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "|_c0|   _c1|_c2|\n",
      "+---+------+---+\n",
      "| TX|   Red| 20|\n",
      "| NV|  Blue| 66|\n",
      "| CO|  Blue| 79|\n",
      "| OR|  Blue| 71|\n",
      "| WA|Yellow| 93|\n",
      "+---+------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "file: String = /tmp/data/csv/df_csv/*\r\n",
       "mnmdfcsv: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// CSV\n",
    "\n",
    "val file = \"/tmp/data/csv/df_csv/*\"\n",
    "\n",
    "val mnmdfcsv= spark.read.format(\"csv\").load(file)\n",
    "\n",
    "mnmdfcsv.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1db0281-ad43-435d-b210-3a79a2af027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State| Color|Count|\n",
      "+-----+------+-----+\n",
      "|   TX|   Red|   20|\n",
      "|   NV|  Blue|   66|\n",
      "|   CO|  Blue|   79|\n",
      "|   OR|  Blue|   71|\n",
      "|   WA|Yellow|   93|\n",
      "+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mnmdfavro: org.apache.spark.sql.DataFrame = [State: string, Color: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// AVRO\n",
    "\n",
    "val mnmdfavro = spark.read.format(\"avro\")\n",
    ".load(\"/tmp/data/avro/df_avro/*\")\n",
    "\n",
    "mnmdfavro.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a96d9c-8f82-450a-8e58-eac1febe22f5",
   "metadata": {},
   "source": [
    "# Capítulo 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e7384-ac90-4f4d-9105-c58c29dd90c4",
   "metadata": {},
   "source": [
    "## Instalar MySQL, descargar driver y cargar datos de BBDD de empleados \n",
    "https://dev.mysql.com/doc/employee/en/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b5c58-c94e-4145-bf81-725ea0512922",
   "metadata": {},
   "source": [
    "### Cargar con spark datos de empleados y departamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5ad33d1-788d-4096-ab44-ff9b4b099e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "// mysql -u root -p < employees.sql (en CMD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "915ed5d0-7b31-487c-9004-e1d52a89d438",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.ClassNotFoundException",
     "evalue": " com.mysql.jdbc.Driver\r",
     "output_type": "error",
     "traceback": [
      "java.lang.ClassNotFoundException: com.mysql.jdbc.Driver\r",
      "  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:72)\r",
      "  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\r",
      "  at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\r",
      "  at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\r",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:102)\r",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:102)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:102)\r",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:38)\r",
      "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:32)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:355)\r",
      "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\r",
      "  at org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\r",
      "  at scala.Option.getOrElse(Option.scala:189)\r",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\r",
      "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:225)\r",
      "  ... 36 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "val employees = spark\n",
    ".read\n",
    ".format(\"jdbc\")\n",
    ".option(\"url\", \"jdbc:mysql://localhost:3306/employees\")\n",
    ".option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    ".option(\"dbtable\", \"employees\")\n",
    ".option(\"user\", \"root\")\n",
    ".option(\"password\", \"root\")\n",
    ".load()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
