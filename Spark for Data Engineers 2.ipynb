{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7810bfe7-9a99-4661-b0ae-398645314a28",
   "metadata": {},
   "source": [
    "# **Reto II**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991460d3-46be-4619-b92b-095c8aa22e71",
   "metadata": {},
   "source": [
    "### 1. Dataset\n",
    "\n",
    "Los datos de origen son proporcionados en un archivos csv:\n",
    "\n",
    "* udfs: dataset con datos de operaciones financieras.\n",
    "\n",
    "### 2. Columnas y significado:\n",
    "\n",
    "* nb: número de referencia de la operación.\n",
    "* contract: identificador de contrato.\n",
    "* udf_ref: identificador de operación de trading.\n",
    "* fmly: familia a la que pertenece la operación financiera.\n",
    "* grp: grupo al que pertenece la operación financiera.\n",
    "* type: tipo de operación financiera.\n",
    "* country: país de origen de la operación.\n",
    "* udf_name: campo informado en el registro.\n",
    "* num_value: valor numérico.\n",
    "* string_value: valor de cadena de caracteres.\n",
    "* date_value: valor de fecha.\n",
    "* data_timestamp_part: marca temporal.\n",
    "* data_date_part: fecha en la que se almacena la información.\n",
    "* source_system: fuente de los datos.\n",
    "\n",
    "### 3. Descripción del problema:\n",
    "\n",
    "Si hacemos una visión general a nuestro conjunto de datos, podemos observar como hay hasta 10 registros (filas) para cada valor de *nb*, donde cada registro solo da información para un valor de *udf_name*. Esto es un gasto innecesario de almacenamiento y computación, además de complicar los futuros cálculos derivados de estos datos. Por esta razón, necesitamos convertir estos registros con el mismo *nb* a un solo registro.\n",
    "\n",
    "Nuestro dataframe final tendrá que contener las siguientes columnas: `nb, M_CCY, M_CLIENT, M_CRDTCHRG, M_DIRECTIAV, M_DISCMARGIN, M_LIQDTYCHRG, M_MVA, M_RVA, M_SELLER, M_SUCURSAL`\n",
    "\n",
    "* nb: debe contener el número de referencia de la operación.\n",
    "* M_CLIENT, M_SELLER, M_CCY, M_SUCURSAL: deben mapear el valor de *string_value*\n",
    "* M_DISCMARGIN, M_DIRECTIAV, M_LIQDTYCHRG, M_CRDTCHRG, , M_MVA, M_RVA: deben mapear el valor de *num_value*\n",
    "\n",
    "\n",
    "Una vez tengamos este resultado, necesitaremos eliminar las operaciones que no tengan informados ninguno de los siguientes campos:\n",
    "\n",
    "M_DISCMARGIN, M_DIRECTIAV, M_LIQDTYCHRG, M_CRDTCHRG, M_MVA, M_RVA, M_SELLER\n",
    "\n",
    "No informados en este caso significa que o son valores nulos, vacíos o 0, en el caso de los campos numéricos.\n",
    "\n",
    "### 4. Reto:\n",
    "\n",
    "* Obtener un dataframe final que contenga las columnas indicadas, con un registro por *nb* y con los valores correctos mapeados.\n",
    "* Las operaciones con los campos M_DISCMARGIN, M_DIRECTIAV, M_LIQDTYCHRG, M_CRDTCHRG, , M_MVA, M_RVA, M_SELLER no informados no deben existir.\n",
    "* Hacerlo de la manera más eficiente posible a nivel computacional.\n",
    "\n",
    "**NOTA:** Cada uno de los pasos descritos en el problema pueden efectuarse en una sola línea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9d0f8-6824-4c6a-9c8d-e189cc012e6a",
   "metadata": {},
   "source": [
    "### Inicialización de SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23122c1-afdd-4b5d-935b-15ba433a036f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://DESKTOP-69418LV.mshome.net:4043\n",
       "SparkContext available as 'sc' (version = 3.1.2, master = local[*], app id = local-1634417367899)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@7d3e5819\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "                        .appName(\"Reto 1\")\n",
    "                        .master(\"local\")\n",
    "                        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6317b0bd-48ef-4598-9391-a04128d51dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "udfs: org.apache.spark.sql.DataFrame = [nb: string, contract: string ... 12 more fields]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val udfs = spark.read.format(\"csv\")\n",
    "                     .option(\"header\", \"true\")\n",
    "                     .option(\"delimiter\", \";\")\n",
    "                     .load(\"./reto2/udfs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e281a-7e09-4627-91ca-eefa8c1ed7dc",
   "metadata": {},
   "source": [
    "### Resultado:\n",
    "\n",
    "**INSTRUCCIONES**: El DataFrame resultante debe almacenarse en la variable `resultado`, sustituyendo el valor `None` por el código que consideréis oportuno. De esta forma podréis comprobar si el resultado es correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca78b82-9656-47f9-8ff3-2dfa4e4eea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val resultado = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c94d779-7dd1-4804-9659-c61ae306a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(resultado.count() == 60)\n",
    "assert(resultado.columns.size == 11)\n",
    "assert(resultado.columns(4) == \"M_DIRECTIAV\")\n",
    "assert(resultado.select(\"M_SELLER\").filter($\"nb\" === 23037162).first.getString(0) == \"AMAM\")\n",
    "assert(resultado.select(\"M_SELLER\").filter($\"nb\" === 19665186).first.getString(0) == \"LB_VSTAVRE\")\n",
    "assert(resultado.select(\"M_RVA\").filter($\"nb\" === 444111222).first.getString(0) == \"8956\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
